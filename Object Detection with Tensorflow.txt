#CNN model for object detection

from pickletools import optimize
from tabnanny import verbose
import unittest
from venv import create
from keras import Optimizer, activations
from keras.src.layers import RandomTranslation
import numpy as np
from numpy import *
import pickle
from fileinput import *
import os
from tkinter import *
import tensorflow as tf
import matplotlib.pyplot as mp
from tensorflow.python import *

#import tf2onnx
#from tf2onnx import *
#import onnx
#from onnx import *


#linking directories for data
BASE_dir = './horse-or-human'
data_dir_horse = os.path.join(BASE_dir, 'horses')
data_dir_human = os.path.join(BASE_dir, 'humans')
print(f"Printing total images for each set")
print(f"There are {len(os.listdir(data_dir_horse))} images of horses")
print(f"There are {len(os.listdir(data_dir_human))} images of humans")
horse_filenames =[os.path.join (data_dir_horse, filename) for filename in os.listdir(data_dir_horse)]
human_filenames =[os.path.join (data_dir_human, filename) for filename in os.listdir(data_dir_human)]
fig, axes = mp.subplots(2, 4, figsize=(10, 5))
fig.suptitle('Images of Horses and Humans', fontsize= 16)

for i, horse_img in enumerate (horse_filenames[:4]):
    img = tf.keras.utils.load_img (horse_img)
    axes[0, i].imshow(img)#printed
    axes[0, i].set_title(f'Example Horse {i}')#printed
for i, human_img in enumerate (human_filenames[:4]):
    img = tf.keras.utils.load_img (human_img)
    axes[1, i].imshow(img)#printed
    axes[1, i].set_title(f'Example Human {i}')#printed
mp.show()
#we have sucessfuly read the data.Now to break it in trainig and validation
def train_val_datasets():
    """
    To create datasets for training and validation.
    Should we use folders to specify each?
    Lets see
       Returns:
        (tf.data.Dataset, tf.data.Dataset): Training and validation datasets.
    """
    train_dataset,val_dataset = tf.keras.utils.image_dataset_from_directory(
    directory = BASE_dir,
    image_size = (300,300),
    batch_size = 64,
    label_mode = "binary",
    validation_split = 0.2,
    subset = "both",
    seed = 40
    )
    return train_dataset, val_dataset
print(f"Seperating Training and Validation dataset")
train_dataset, val_dataset = train_val_datasets()
for images, labels in train_dataset.take(1):
    example_batch_images = images
    example_batch_labels = labels
print(f"Maximum pixel value of images: {np.max(example_batch_images)}\n")
print(f"Shape of batch of images: {example_batch_images.shape}\n")
print(f"Shape of batch of labels: {example_batch_labels.shape}\n")
#testing code
#unittests.test_train_val_datasets(train_val_datasets)


print(f"Now model for convolution would be created")
def cr_model():#create after consulting research papers
    """
    Classification of horses and humans
    Returns:
      tf.keras.Model: The model that will be trained to classify cats and dogs.
    """
    
    model = tf.keras.models.Sequential([
        tf.keras.Input(shape=(300,300,3)),
        tf.keras.layers.Rescaling(scale=1./255),
        tf.keras.layers.Conv2D(32,(3,3), activation='relu'),
        tf.keras.layers.Conv2D(32,(3,3), activation='relu'),
        tf.keras.layers.MaxPooling2D(2,2),
        tf.keras.layers.Conv2D(64,(3,3), activation='relu'),
        tf.keras.layers.Conv2D(64,(3,3), activation='relu'),
        tf.keras.layers.MaxPooling2D(2,2),
        tf.keras.layers.Conv2D(128,(3,3), activation='relu'),
        tf.keras.layers.Conv2D(128,(3,3), activation='relu'),
        tf.keras.layers.Conv2D(128,(3,3), activation='relu'),
        tf.keras.layers.MaxPooling2D(2,2),
        tf.keras.layers.Conv2D(256,(3,3), activation='relu'),
        tf.keras.layers.Conv2D(256,(3,3), activation='relu'),
        tf.keras.layers.Conv2D(256,(3,3), activation='relu'),
        tf.keras.layers.MaxPooling2D(2,2),
        tf.keras.layers.Conv2D(512,(3,3), activation='relu'),
        tf.keras.layers.Conv2D(512,(3,3), activation='relu'),
        tf.keras.layers.Conv2D(512,(3,3), activation='relu'),
        tf.keras.layers.MaxPooling2D(2,2),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(512, activation='relu'),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(512, activation='relu'),#hidden units
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(512, activation='relu'),#hidden units
        tf.keras.layers.Dense(1, activation='sigmoid')
        ])
    return model
#defining plotting scheme
def plot_loss_acc(history):
    """
    For validation and train dataset accuracy
    """
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(len(acc))

    fig, ax = mp.subplots(1, 2, figsize = (12,6))
    ax[0].plot(epochs, acc, 'bo', label='Training Accuracy')
    ax[0].plot(epochs, val_acc, 'b', label='Validation Accuracy')
    ax[0].set_title('Training and Validation accuracy')
    ax[0].set_xlabel('epochs')
    ax[0].set_ylabel('accuracy')
    ax[0].legend()


    ax[1].plot(epochs, loss, 'bo', label='Training Loss')
    ax[1].plot(epochs, val_loss, 'b', label='Validation Loss')
    ax[1].set_title('Training and Validation loss')
    ax[1].set_xlabel('epochs')
    ax[1].set_ylabel('accuracy')
    ax[1].legend()

    mp.show()



#now we would introduce Data Augmentation for different aspects of data fed to the model
data_augmentation =tf.keras.Sequential ([
    tf.keras.Input(shape=(300,300,3)),
    tf.keras.layers.RandomFlip("horizontal"),
    tf.keras.layers.RandomRotation(0.2, fill_mode='nearest'),
    tf.keras.layers.RandomTranslation(0.2, 0.2, fill_mode='nearest'),
    tf.keras.layers.RandomZoom(0.2, fill_mode='nearest')    
    ]) 

#model run 
model_unaurgmented = cr_model()
model_augmented = tf.keras.models.Sequential([
    data_augmentation,
    model_unaurgmented
    ])
#compiling the model
model_augmented.compile(loss='binary_crossentropy',
                        optimizer=tf.keras.optimizers.RMSprop(learning_rate = 1e-4),
                        metrics=['accuracy']
                        )

EPOCHS = 20
#running model
print(f"Model executed")
history = model_augmented.fit(
    train_dataset,
    epochs=EPOCHS,
    verbose=2,
    validation_data=val_dataset
    )
model_augmented.save('horses_v_humans.keras')
plot_loss_acc(history)